output_dir = /home/ljw/wuqiang/motif_ai/results
seed = 63036
# device = cpu # cpu, cuda, if not specified, use cuda if available
log = WARNING

[dataset]
test_ratio = 0.1
validation_ratio = 0.1

[data loader]
batch_size = 1000

[optimizer]
optimizer = adamw_torch # adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision, adafactor
learning_rate = 0.001

[scheduler]
scheduler = linear # linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup, inverse_sqrt, reduce_lr_on_plateau, cosine_with_min_lr, warmup_stable_decay
num_epochs = 30.0
warmup_ratio = 0.05

[roformer]
hidden_size = 256 # model embedding dimension
num_hidden_layers = 3 # number of EncoderLayer
num_attention_heads = 4 # number of attention heads
intermediate_size = 1024 # FeedForward intermediate dimension size
hidden_dropout_prob = 0.1 # The dropout probability for all fully connected layers in the embeddings, encoder, and pooler
attention_probs_dropout_prob = 0.1 # The dropout ratio for the attention probabilities
max_position_embeddings = 32 # The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 1536).
